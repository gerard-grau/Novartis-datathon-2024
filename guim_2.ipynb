{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Required Libraries\n",
    "Import necessary libraries such as pandas, numpy, tensorflow, and keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Preprocess Data\n",
    "Load the train_data.csv file and preprocess the data, including handling missing values and scaling features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('data/train_data.csv')\n",
    "train_df.replace(-1.0, np.nan, inplace=True)\n",
    "train_df.replace('-1', np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['che_pc_usd', 'che_perc_gdp', 'insurance_perc_che', 'population',\n",
       "       'prev_perc', 'price_month', 'price_unit', 'public_perc_che', 'target'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric_cols = train_df.select_dtypes(include=[np.number]).columns\n",
    "numeric_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['brand', 'che_pc_usd', 'che_perc_gdp', 'cluster_nl', 'corporation',\n",
       "       'country', 'launch_date', 'date', 'drug_id', 'ind_launch_date',\n",
       "       'indication', 'insurance_perc_che', 'population', 'prev_perc',\n",
       "       'price_month', 'price_unit', 'public_perc_che', 'therapeutic_area',\n",
       "       'target'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[numeric_cols] = train_df[numeric_cols].fillna(train_df[numeric_cols].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_columns = ['launch_date', 'date', 'ind_launch_date']\n",
    "for col in date_columns:\n",
    "    train_df[col] = pd.to_datetime(train_df[col], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_enc_columns = ['brand', 'corporation', 'country', 'therapeutic_area', 'drug_id']\n",
    "label_encoders = {}\n",
    "for col in label_enc_columns:\n",
    "    le = LabelEncoder()\n",
    "    train_df[col] = le.fit_transform(train_df[col].astype(str))\n",
    "    label_encoders[col] = le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brand</th>\n",
       "      <th>che_pc_usd</th>\n",
       "      <th>che_perc_gdp</th>\n",
       "      <th>cluster_nl</th>\n",
       "      <th>corporation</th>\n",
       "      <th>country</th>\n",
       "      <th>launch_date</th>\n",
       "      <th>date</th>\n",
       "      <th>drug_id</th>\n",
       "      <th>ind_launch_date</th>\n",
       "      <th>indication</th>\n",
       "      <th>insurance_perc_che</th>\n",
       "      <th>population</th>\n",
       "      <th>prev_perc</th>\n",
       "      <th>price_month</th>\n",
       "      <th>price_unit</th>\n",
       "      <th>public_perc_che</th>\n",
       "      <th>therapeutic_area</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>113</td>\n",
       "      <td>1.209114</td>\n",
       "      <td>1.665879</td>\n",
       "      <td>BRAND_354E_COUNTRY_88A3</td>\n",
       "      <td>116</td>\n",
       "      <td>28</td>\n",
       "      <td>2014-06-01</td>\n",
       "      <td>2014-06-01</td>\n",
       "      <td>121</td>\n",
       "      <td>NaT</td>\n",
       "      <td>['IND_C3B6']</td>\n",
       "      <td>1.893333</td>\n",
       "      <td>1.008039</td>\n",
       "      <td>0.028367</td>\n",
       "      <td>1.006444</td>\n",
       "      <td>1.013784</td>\n",
       "      <td>1.835821</td>\n",
       "      <td>10</td>\n",
       "      <td>1.000784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>223</td>\n",
       "      <td>1.472378</td>\n",
       "      <td>1.753338</td>\n",
       "      <td>BRAND_626D_COUNTRY_8B47</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>2014-06-01</td>\n",
       "      <td>2014-06-01</td>\n",
       "      <td>223</td>\n",
       "      <td>2014-09-01</td>\n",
       "      <td>['IND_1590', 'IND_ECAC']</td>\n",
       "      <td>1.546667</td>\n",
       "      <td>1.023562</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>1.121036</td>\n",
       "      <td>1.626677</td>\n",
       "      <td>1.835821</td>\n",
       "      <td>9</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>155</td>\n",
       "      <td>1.209114</td>\n",
       "      <td>1.665879</td>\n",
       "      <td>BRAND_45D9_COUNTRY_88A3</td>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "      <td>2014-06-01</td>\n",
       "      <td>2014-06-01</td>\n",
       "      <td>236</td>\n",
       "      <td>NaT</td>\n",
       "      <td>['IND_B2EF']</td>\n",
       "      <td>1.893333</td>\n",
       "      <td>1.008039</td>\n",
       "      <td>0.001502</td>\n",
       "      <td>1.121036</td>\n",
       "      <td>3.144874</td>\n",
       "      <td>1.835821</td>\n",
       "      <td>9</td>\n",
       "      <td>1.002258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>489</td>\n",
       "      <td>1.851280</td>\n",
       "      <td>2.051770</td>\n",
       "      <td>BRAND_D724_COUNTRY_445D</td>\n",
       "      <td>55</td>\n",
       "      <td>13</td>\n",
       "      <td>2014-06-01</td>\n",
       "      <td>2014-06-01</td>\n",
       "      <td>25</td>\n",
       "      <td>NaT</td>\n",
       "      <td>['IND_BAFB']</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.253186</td>\n",
       "      <td>0.001304</td>\n",
       "      <td>1.121036</td>\n",
       "      <td>1.213446</td>\n",
       "      <td>1.805970</td>\n",
       "      <td>7</td>\n",
       "      <td>1.068761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>161</td>\n",
       "      <td>1.791199</td>\n",
       "      <td>2.059130</td>\n",
       "      <td>BRAND_4887_COUNTRY_D8B0</td>\n",
       "      <td>34</td>\n",
       "      <td>43</td>\n",
       "      <td>2014-06-01</td>\n",
       "      <td>2014-06-01</td>\n",
       "      <td>149</td>\n",
       "      <td>NaT</td>\n",
       "      <td>['IND_3F31']</td>\n",
       "      <td>2.013333</td>\n",
       "      <td>1.639352</td>\n",
       "      <td>0.054467</td>\n",
       "      <td>1.018589</td>\n",
       "      <td>1.008708</td>\n",
       "      <td>1.880597</td>\n",
       "      <td>7</td>\n",
       "      <td>1.036312</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   brand  che_pc_usd  che_perc_gdp               cluster_nl  corporation  \\\n",
       "0    113    1.209114      1.665879  BRAND_354E_COUNTRY_88A3          116   \n",
       "1    223    1.472378      1.753338  BRAND_626D_COUNTRY_8B47            0   \n",
       "2    155    1.209114      1.665879  BRAND_45D9_COUNTRY_88A3           28   \n",
       "3    489    1.851280      2.051770  BRAND_D724_COUNTRY_445D           55   \n",
       "4    161    1.791199      2.059130  BRAND_4887_COUNTRY_D8B0           34   \n",
       "\n",
       "   country launch_date       date  drug_id ind_launch_date  \\\n",
       "0       28  2014-06-01 2014-06-01      121             NaT   \n",
       "1       30  2014-06-01 2014-06-01      223      2014-09-01   \n",
       "2       28  2014-06-01 2014-06-01      236             NaT   \n",
       "3       13  2014-06-01 2014-06-01       25             NaT   \n",
       "4       43  2014-06-01 2014-06-01      149             NaT   \n",
       "\n",
       "                 indication  insurance_perc_che  population  prev_perc  \\\n",
       "0              ['IND_C3B6']            1.893333    1.008039   0.028367   \n",
       "1  ['IND_1590', 'IND_ECAC']            1.546667    1.023562   0.000047   \n",
       "2              ['IND_B2EF']            1.893333    1.008039   0.001502   \n",
       "3              ['IND_BAFB']            1.000000    1.253186   0.001304   \n",
       "4              ['IND_3F31']            2.013333    1.639352   0.054467   \n",
       "\n",
       "   price_month  price_unit  public_perc_che  therapeutic_area    target  \n",
       "0     1.006444    1.013784         1.835821                10  1.000784  \n",
       "1     1.121036    1.626677         1.835821                 9  1.000000  \n",
       "2     1.121036    3.144874         1.835821                 9  1.002258  \n",
       "3     1.121036    1.213446         1.805970                 7  1.068761  \n",
       "4     1.018589    1.008708         1.880597                 7  1.036312  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rolling features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(df, window_size = 3, label=None):\n",
    "    \"\"\"\n",
    "    Creates time series features from datetime index\n",
    "    \"\"\"\n",
    "    # Extract features from 'launch_date'\n",
    "    df['launch_year'] = df['launch_date'].dt.year\n",
    "    df['launch_month'] = df['launch_date'].dt.month\n",
    "    df['launch_day'] = df['launch_date'].dt.day\n",
    "    df['launch_dayofweek'] = df['launch_date'].dt.dayofweek  # 0 = Monday\n",
    "    df['launch_is_weekend'] = df['launch_dayofweek'] >= 5\n",
    "\n",
    "    # Extract features from 'date'\n",
    "    df['date_year'] = df['date'].dt.year\n",
    "    df['date_month'] = df['date'].dt.month\n",
    "    df['date_day'] = df['date'].dt.day\n",
    "    df['date_dayofweek'] = df['date'].dt.dayofweek\n",
    "    df['date_is_weekend'] = df['date_dayofweek'] >= 5\n",
    "\n",
    "    # Remove column target frmo numeric_cols\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    if label in numeric_cols:\n",
    "        numeric_cols = numeric_cols.drop(label)\n",
    "\n",
    "    # Create rolling features\n",
    "    for col in numeric_cols:\n",
    "        df[f'{col}_rolling_mean'] = df[col].rolling(window=window_size).mean()\n",
    "        df[f'{col}_rolling_sum'] = df[col].rolling(window=window_size).sum()\n",
    "        df[f'{col}_rolling_std'] = df[col].rolling(window=window_size).std()\n",
    "        df[f'{col}_rolling_min'] = df[col].rolling(window=window_size).min()\n",
    "        df[f'{col}_rolling_max'] = df[col].rolling(window=window_size).max()\n",
    "        df[f'{col}_lag_1'] = df[col].shift(1)\n",
    "        df[f'{col}_lag_2'] = df[col].shift(2)\n",
    "        df[f'{col}_lag_3'] = df[col].shift(3)\n",
    "\n",
    "    # Interaction features\n",
    "    df['launch_month_times_date_month'] = df['launch_month'] * df['date_month']\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17313/1691776604.py:33: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_lag_3'] = df[col].shift(3)\n",
      "/tmp/ipykernel_17313/1691776604.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_mean'] = df[col].rolling(window=window_size).mean()\n",
      "/tmp/ipykernel_17313/1691776604.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_sum'] = df[col].rolling(window=window_size).sum()\n",
      "/tmp/ipykernel_17313/1691776604.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_std'] = df[col].rolling(window=window_size).std()\n",
      "/tmp/ipykernel_17313/1691776604.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_min'] = df[col].rolling(window=window_size).min()\n",
      "/tmp/ipykernel_17313/1691776604.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_max'] = df[col].rolling(window=window_size).max()\n",
      "/tmp/ipykernel_17313/1691776604.py:31: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_lag_1'] = df[col].shift(1)\n",
      "/tmp/ipykernel_17313/1691776604.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_lag_2'] = df[col].shift(2)\n",
      "/tmp/ipykernel_17313/1691776604.py:33: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_lag_3'] = df[col].shift(3)\n",
      "/tmp/ipykernel_17313/1691776604.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_mean'] = df[col].rolling(window=window_size).mean()\n",
      "/tmp/ipykernel_17313/1691776604.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_sum'] = df[col].rolling(window=window_size).sum()\n",
      "/tmp/ipykernel_17313/1691776604.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_std'] = df[col].rolling(window=window_size).std()\n",
      "/tmp/ipykernel_17313/1691776604.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_min'] = df[col].rolling(window=window_size).min()\n",
      "/tmp/ipykernel_17313/1691776604.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_max'] = df[col].rolling(window=window_size).max()\n",
      "/tmp/ipykernel_17313/1691776604.py:31: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_lag_1'] = df[col].shift(1)\n",
      "/tmp/ipykernel_17313/1691776604.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_lag_2'] = df[col].shift(2)\n",
      "/tmp/ipykernel_17313/1691776604.py:33: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_lag_3'] = df[col].shift(3)\n",
      "/tmp/ipykernel_17313/1691776604.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_mean'] = df[col].rolling(window=window_size).mean()\n",
      "/tmp/ipykernel_17313/1691776604.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_sum'] = df[col].rolling(window=window_size).sum()\n",
      "/tmp/ipykernel_17313/1691776604.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_std'] = df[col].rolling(window=window_size).std()\n",
      "/tmp/ipykernel_17313/1691776604.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_min'] = df[col].rolling(window=window_size).min()\n",
      "/tmp/ipykernel_17313/1691776604.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_max'] = df[col].rolling(window=window_size).max()\n",
      "/tmp/ipykernel_17313/1691776604.py:31: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_lag_1'] = df[col].shift(1)\n",
      "/tmp/ipykernel_17313/1691776604.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_lag_2'] = df[col].shift(2)\n",
      "/tmp/ipykernel_17313/1691776604.py:33: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_lag_3'] = df[col].shift(3)\n",
      "/tmp/ipykernel_17313/1691776604.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_mean'] = df[col].rolling(window=window_size).mean()\n",
      "/tmp/ipykernel_17313/1691776604.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_sum'] = df[col].rolling(window=window_size).sum()\n",
      "/tmp/ipykernel_17313/1691776604.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_std'] = df[col].rolling(window=window_size).std()\n",
      "/tmp/ipykernel_17313/1691776604.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_min'] = df[col].rolling(window=window_size).min()\n",
      "/tmp/ipykernel_17313/1691776604.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_max'] = df[col].rolling(window=window_size).max()\n",
      "/tmp/ipykernel_17313/1691776604.py:31: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_lag_1'] = df[col].shift(1)\n",
      "/tmp/ipykernel_17313/1691776604.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_lag_2'] = df[col].shift(2)\n",
      "/tmp/ipykernel_17313/1691776604.py:33: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_lag_3'] = df[col].shift(3)\n",
      "/tmp/ipykernel_17313/1691776604.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_mean'] = df[col].rolling(window=window_size).mean()\n",
      "/tmp/ipykernel_17313/1691776604.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_sum'] = df[col].rolling(window=window_size).sum()\n",
      "/tmp/ipykernel_17313/1691776604.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_std'] = df[col].rolling(window=window_size).std()\n",
      "/tmp/ipykernel_17313/1691776604.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_min'] = df[col].rolling(window=window_size).min()\n",
      "/tmp/ipykernel_17313/1691776604.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_max'] = df[col].rolling(window=window_size).max()\n",
      "/tmp/ipykernel_17313/1691776604.py:31: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_lag_1'] = df[col].shift(1)\n",
      "/tmp/ipykernel_17313/1691776604.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_lag_2'] = df[col].shift(2)\n",
      "/tmp/ipykernel_17313/1691776604.py:33: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_lag_3'] = df[col].shift(3)\n",
      "/tmp/ipykernel_17313/1691776604.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_mean'] = df[col].rolling(window=window_size).mean()\n",
      "/tmp/ipykernel_17313/1691776604.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_sum'] = df[col].rolling(window=window_size).sum()\n",
      "/tmp/ipykernel_17313/1691776604.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_std'] = df[col].rolling(window=window_size).std()\n",
      "/tmp/ipykernel_17313/1691776604.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_min'] = df[col].rolling(window=window_size).min()\n",
      "/tmp/ipykernel_17313/1691776604.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_max'] = df[col].rolling(window=window_size).max()\n",
      "/tmp/ipykernel_17313/1691776604.py:31: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_lag_1'] = df[col].shift(1)\n",
      "/tmp/ipykernel_17313/1691776604.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_lag_2'] = df[col].shift(2)\n",
      "/tmp/ipykernel_17313/1691776604.py:33: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_lag_3'] = df[col].shift(3)\n",
      "/tmp/ipykernel_17313/1691776604.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_mean'] = df[col].rolling(window=window_size).mean()\n",
      "/tmp/ipykernel_17313/1691776604.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_sum'] = df[col].rolling(window=window_size).sum()\n",
      "/tmp/ipykernel_17313/1691776604.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_std'] = df[col].rolling(window=window_size).std()\n",
      "/tmp/ipykernel_17313/1691776604.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_min'] = df[col].rolling(window=window_size).min()\n",
      "/tmp/ipykernel_17313/1691776604.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_max'] = df[col].rolling(window=window_size).max()\n",
      "/tmp/ipykernel_17313/1691776604.py:31: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_lag_1'] = df[col].shift(1)\n",
      "/tmp/ipykernel_17313/1691776604.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_lag_2'] = df[col].shift(2)\n",
      "/tmp/ipykernel_17313/1691776604.py:33: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_lag_3'] = df[col].shift(3)\n",
      "/tmp/ipykernel_17313/1691776604.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_mean'] = df[col].rolling(window=window_size).mean()\n",
      "/tmp/ipykernel_17313/1691776604.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_sum'] = df[col].rolling(window=window_size).sum()\n",
      "/tmp/ipykernel_17313/1691776604.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_std'] = df[col].rolling(window=window_size).std()\n",
      "/tmp/ipykernel_17313/1691776604.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_min'] = df[col].rolling(window=window_size).min()\n",
      "/tmp/ipykernel_17313/1691776604.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_max'] = df[col].rolling(window=window_size).max()\n",
      "/tmp/ipykernel_17313/1691776604.py:31: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_lag_1'] = df[col].shift(1)\n",
      "/tmp/ipykernel_17313/1691776604.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_lag_2'] = df[col].shift(2)\n",
      "/tmp/ipykernel_17313/1691776604.py:33: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_lag_3'] = df[col].shift(3)\n",
      "/tmp/ipykernel_17313/1691776604.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_mean'] = df[col].rolling(window=window_size).mean()\n",
      "/tmp/ipykernel_17313/1691776604.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_sum'] = df[col].rolling(window=window_size).sum()\n",
      "/tmp/ipykernel_17313/1691776604.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_std'] = df[col].rolling(window=window_size).std()\n",
      "/tmp/ipykernel_17313/1691776604.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_min'] = df[col].rolling(window=window_size).min()\n",
      "/tmp/ipykernel_17313/1691776604.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_max'] = df[col].rolling(window=window_size).max()\n",
      "/tmp/ipykernel_17313/1691776604.py:31: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_lag_1'] = df[col].shift(1)\n",
      "/tmp/ipykernel_17313/1691776604.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_lag_2'] = df[col].shift(2)\n",
      "/tmp/ipykernel_17313/1691776604.py:33: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_lag_3'] = df[col].shift(3)\n",
      "/tmp/ipykernel_17313/1691776604.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_mean'] = df[col].rolling(window=window_size).mean()\n",
      "/tmp/ipykernel_17313/1691776604.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_sum'] = df[col].rolling(window=window_size).sum()\n",
      "/tmp/ipykernel_17313/1691776604.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_std'] = df[col].rolling(window=window_size).std()\n",
      "/tmp/ipykernel_17313/1691776604.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_min'] = df[col].rolling(window=window_size).min()\n",
      "/tmp/ipykernel_17313/1691776604.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_max'] = df[col].rolling(window=window_size).max()\n",
      "/tmp/ipykernel_17313/1691776604.py:31: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_lag_1'] = df[col].shift(1)\n",
      "/tmp/ipykernel_17313/1691776604.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_lag_2'] = df[col].shift(2)\n",
      "/tmp/ipykernel_17313/1691776604.py:33: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_lag_3'] = df[col].shift(3)\n",
      "/tmp/ipykernel_17313/1691776604.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_mean'] = df[col].rolling(window=window_size).mean()\n",
      "/tmp/ipykernel_17313/1691776604.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_sum'] = df[col].rolling(window=window_size).sum()\n",
      "/tmp/ipykernel_17313/1691776604.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_std'] = df[col].rolling(window=window_size).std()\n",
      "/tmp/ipykernel_17313/1691776604.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_min'] = df[col].rolling(window=window_size).min()\n",
      "/tmp/ipykernel_17313/1691776604.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_max'] = df[col].rolling(window=window_size).max()\n",
      "/tmp/ipykernel_17313/1691776604.py:31: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_lag_1'] = df[col].shift(1)\n",
      "/tmp/ipykernel_17313/1691776604.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_lag_2'] = df[col].shift(2)\n",
      "/tmp/ipykernel_17313/1691776604.py:33: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_lag_3'] = df[col].shift(3)\n",
      "/tmp/ipykernel_17313/1691776604.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_mean'] = df[col].rolling(window=window_size).mean()\n",
      "/tmp/ipykernel_17313/1691776604.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_sum'] = df[col].rolling(window=window_size).sum()\n",
      "/tmp/ipykernel_17313/1691776604.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_std'] = df[col].rolling(window=window_size).std()\n",
      "/tmp/ipykernel_17313/1691776604.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_min'] = df[col].rolling(window=window_size).min()\n",
      "/tmp/ipykernel_17313/1691776604.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_rolling_max'] = df[col].rolling(window=window_size).max()\n",
      "/tmp/ipykernel_17313/1691776604.py:31: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_lag_1'] = df[col].shift(1)\n",
      "/tmp/ipykernel_17313/1691776604.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_lag_2'] = df[col].shift(2)\n",
      "/tmp/ipykernel_17313/1691776604.py:33: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}_lag_3'] = df[col].shift(3)\n",
      "/tmp/ipykernel_17313/1691776604.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['launch_month_times_date_month'] = df['launch_month'] * df['date_month']\n"
     ]
    }
   ],
   "source": [
    "train_df_rolling = create_features(train_df, label= 'target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17313/3539690154.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df['mean_country'] = train_df['country'].map(df_country['target_mean'])\n",
      "/tmp/ipykernel_17313/3539690154.py:38: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df['median_country'] = train_df['country'].map(df_country['target_median'])\n",
      "/tmp/ipykernel_17313/3539690154.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df['std_country'] = train_df['country'].map(df_country['target_std'])\n",
      "/tmp/ipykernel_17313/3539690154.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df['min_country'] = train_df['country'].map(df_country['target_min'])\n",
      "/tmp/ipykernel_17313/3539690154.py:41: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df['max_country'] = train_df['country'].map(df_country['target_max'])\n",
      "/tmp/ipykernel_17313/3539690154.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df['mean_brand'] = train_df['brand'].map(df_brand['target_mean'])\n",
      "/tmp/ipykernel_17313/3539690154.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df['median_brand'] = train_df['brand'].map(df_brand['target_median'])\n",
      "/tmp/ipykernel_17313/3539690154.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df['std_brand'] = train_df['brand'].map(df_brand['target_std'])\n",
      "/tmp/ipykernel_17313/3539690154.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df['min_brand'] = train_df['brand'].map(df_brand['target_min'])\n",
      "/tmp/ipykernel_17313/3539690154.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df['max_brand'] = train_df['brand'].map(df_brand['target_max'])\n",
      "/tmp/ipykernel_17313/3539690154.py:49: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df['mean_drug_id'] = train_df['drug_id'].map(df_drug_id['target_mean'])\n",
      "/tmp/ipykernel_17313/3539690154.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df['median_drug_id'] = train_df['drug_id'].map(df_drug_id['target_median'])\n",
      "/tmp/ipykernel_17313/3539690154.py:51: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df['std_drug_id'] = train_df['drug_id'].map(df_drug_id['target_std'])\n",
      "/tmp/ipykernel_17313/3539690154.py:52: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df['min_drug_id'] = train_df['drug_id'].map(df_drug_id['target_min'])\n",
      "/tmp/ipykernel_17313/3539690154.py:53: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df['max_drug_id'] = train_df['drug_id'].map(df_drug_id['target_max'])\n",
      "/tmp/ipykernel_17313/3539690154.py:55: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df['mean_country_brand'] = train_df[['country', 'brand']].apply(lambda x: df_country_brand.loc[tuple(x), 'target_mean'] if tuple(x) in df_country_brand.index else np.nan, axis=1)\n",
      "/tmp/ipykernel_17313/3539690154.py:56: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df['median_country_brand'] = train_df[['country', 'brand']].apply(lambda x: df_country_brand.loc[tuple(x), 'target_median'] if tuple(x) in df_country_brand.index else np.nan, axis=1)\n",
      "/tmp/ipykernel_17313/3539690154.py:57: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df['std_country_brand'] = train_df[['country', 'brand']].apply(lambda x: df_country_brand.loc[tuple(x), 'target_std'] if tuple(x) in df_country_brand.index else np.nan, axis=1)\n",
      "/tmp/ipykernel_17313/3539690154.py:58: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df['min_country_brand'] = train_df[['country', 'brand']].apply(lambda x: df_country_brand.loc[tuple(x), 'target_min'] if tuple(x) in df_country_brand.index else np.nan, axis=1)\n",
      "/tmp/ipykernel_17313/3539690154.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df['max_country_brand'] = train_df[['country', 'brand']].apply(lambda x: df_country_brand.loc[tuple(x), 'target_max'] if tuple(x) in df_country_brand.index else np.nan, axis=1)\n",
      "/tmp/ipykernel_17313/3539690154.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df['mean_brand_drug_id'] = train_df[['brand', 'drug_id']].apply(lambda x: df_brand_drug_id.loc[tuple(x), 'target_mean'] if tuple(x) in df_brand_drug_id.index else np.nan, axis=1)\n",
      "/tmp/ipykernel_17313/3539690154.py:62: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df['median_brand_drug_id'] = train_df[['brand', 'drug_id']].apply(lambda x: df_brand_drug_id.loc[tuple(x), 'target_median'] if tuple(x) in df_brand_drug_id.index else np.nan, axis=1)\n",
      "/tmp/ipykernel_17313/3539690154.py:63: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df['std_brand_drug_id'] = train_df[['brand', 'drug_id']].apply(lambda x: df_brand_drug_id.loc[tuple(x), 'target_std'] if tuple(x) in df_brand_drug_id.index else np.nan, axis=1)\n",
      "/tmp/ipykernel_17313/3539690154.py:64: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df['min_brand_drug_id'] = train_df[['brand', 'drug_id']].apply(lambda x: df_brand_drug_id.loc[tuple(x), 'target_min'] if tuple(x) in df_brand_drug_id.index else np.nan, axis=1)\n",
      "/tmp/ipykernel_17313/3539690154.py:65: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df['max_brand_drug_id'] = train_df[['brand', 'drug_id']].apply(lambda x: df_brand_drug_id.loc[tuple(x), 'target_max'] if tuple(x) in df_brand_drug_id.index else np.nan, axis=1)\n",
      "/tmp/ipykernel_17313/3539690154.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df['mean_country_drug_id'] = train_df[['country', 'drug_id']].apply(lambda x: df_country_drug_id.loc[tuple(x), 'target_mean'] if tuple(x) in df_country_drug_id.index else np.nan, axis=1)\n",
      "/tmp/ipykernel_17313/3539690154.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df['median_country_drug_id'] = train_df[['country', 'drug_id']].apply(lambda x: df_country_drug_id.loc[tuple(x), 'target_median'] if tuple(x) in df_country_drug_id.index else np.nan, axis=1)\n",
      "/tmp/ipykernel_17313/3539690154.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df['std_country_drug_id'] = train_df[['country', 'drug_id']].apply(lambda x: df_country_drug_id.loc[tuple(x), 'target_std'] if tuple(x) in df_country_drug_id.index else np.nan, axis=1)\n",
      "/tmp/ipykernel_17313/3539690154.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df['min_country_drug_id'] = train_df[['country', 'drug_id']].apply(lambda x: df_country_drug_id.loc[tuple(x), 'target_min'] if tuple(x) in df_country_drug_id.index else np.nan, axis=1)\n",
      "/tmp/ipykernel_17313/3539690154.py:71: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df['max_country_drug_id'] = train_df[['country', 'drug_id']].apply(lambda x: df_country_drug_id.loc[tuple(x), 'target_max'] if tuple(x) in df_country_drug_id.index else np.nan, axis=1)\n",
      "/tmp/ipykernel_17313/3539690154.py:73: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df['mean_country_brand_drug_id'] = train_df[['country', 'brand', 'drug_id']].apply(lambda x: df_country_brand_drug_id.loc[tuple(x), 'target_mean'] if tuple(x) in df_country_brand_drug_id.index else np.nan, axis=1)\n",
      "/tmp/ipykernel_17313/3539690154.py:74: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df['median_country_brand_drug_id'] = train_df[['country', 'brand', 'drug_id']].apply(lambda x: df_country_brand_drug_id.loc[tuple(x), 'target_median'] if tuple(x) in df_country_brand_drug_id.index else np.nan, axis=1)\n",
      "/tmp/ipykernel_17313/3539690154.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df['std_country_brand_drug_id'] = train_df[['country', 'brand', 'drug_id']].apply(lambda x: df_country_brand_drug_id.loc[tuple(x), 'target_std'] if tuple(x) in df_country_brand_drug_id.index else np.nan, axis=1)\n",
      "/tmp/ipykernel_17313/3539690154.py:76: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df['min_country_brand_drug_id'] = train_df[['country', 'brand', 'drug_id']].apply(lambda x: df_country_brand_drug_id.loc[tuple(x), 'target_min'] if tuple(x) in df_country_brand_drug_id.index else np.nan, axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   brand  che_pc_usd  che_perc_gdp               cluster_nl  corporation  \\\n",
      "0    113    1.209114      1.665879  BRAND_354E_COUNTRY_88A3          116   \n",
      "1    223    1.472378      1.753338  BRAND_626D_COUNTRY_8B47            0   \n",
      "2    155    1.209114      1.665879  BRAND_45D9_COUNTRY_88A3           28   \n",
      "3    489    1.851280      2.051770  BRAND_D724_COUNTRY_445D           55   \n",
      "4    161    1.791199      2.059130  BRAND_4887_COUNTRY_D8B0           34   \n",
      "\n",
      "   country launch_date       date  drug_id ind_launch_date  ...  \\\n",
      "0       28  2014-06-01 2014-06-01      121             NaT  ...   \n",
      "1       30  2014-06-01 2014-06-01      223      2014-09-01  ...   \n",
      "2       28  2014-06-01 2014-06-01      236             NaT  ...   \n",
      "3       13  2014-06-01 2014-06-01       25             NaT  ...   \n",
      "4       43  2014-06-01 2014-06-01      149             NaT  ...   \n",
      "\n",
      "  mean_country_drug_id  median_country_drug_id  std_country_drug_id  \\\n",
      "0             1.033013                1.034500             0.010762   \n",
      "1             1.055072                1.053636             0.027736   \n",
      "2             1.034781                1.033613             0.015483   \n",
      "3             2.245871                2.416240             0.447509   \n",
      "4             1.390586                1.392337             0.124172   \n",
      "\n",
      "   min_country_drug_id  max_country_drug_id  mean_country_brand_drug_id  \\\n",
      "0             1.000784             1.052647                    1.033013   \n",
      "1             1.000000             1.118436                    1.055072   \n",
      "2             1.002258             1.066490                    1.034781   \n",
      "3             1.068761             2.774457                    2.245871   \n",
      "4             1.036312             1.620554                    1.390586   \n",
      "\n",
      "   median_country_brand_drug_id  std_country_brand_drug_id  \\\n",
      "0                      1.034500                   0.010762   \n",
      "1                      1.053636                   0.027736   \n",
      "2                      1.033613                   0.015483   \n",
      "3                      2.416240                   0.447509   \n",
      "4                      1.392337                   0.124172   \n",
      "\n",
      "   min_country_brand_drug_id  max_country_brand_drug_id  \n",
      "0                   1.000784                   1.052647  \n",
      "1                   1.000000                   1.118436  \n",
      "2                   1.002258                   1.066490  \n",
      "3                   1.068761                   2.774457  \n",
      "4                   1.036312                   1.620554  \n",
      "\n",
      "[5 rows x 233 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17313/3539690154.py:77: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_df['max_country_brand_drug_id'] = train_df[['country', 'brand', 'drug_id']].apply(lambda x: df_country_brand_drug_id.loc[tuple(x), 'target_max'] if tuple(x) in df_country_brand_drug_id.index else np.nan, axis=1)\n"
     ]
    }
   ],
   "source": [
    "def create_aggregates(df, group_columns, target='target'):\n",
    "    \"\"\"\n",
    "    Creates aggregate statistics (mean, median, std, min, max) for launches by specified group_columns.\n",
    "    \"\"\"\n",
    "    # Group by the specified columns and compute aggregate statistics for the target variable\n",
    "    grouped = df.groupby(group_columns)[target].agg(['mean', 'median', 'std', 'min', 'max'])\n",
    "\n",
    "    # Flatten the multi-level columns and create new column names\n",
    "    grouped.columns = [f'{target}_{agg}' for agg in grouped.columns]\n",
    "\n",
    "    # Return the aggregates (they will be joined to the original dataframe later)\n",
    "    return grouped\n",
    "\n",
    "# 1. Aggregate by 'country'\n",
    "df_country = create_aggregates(train_df, group_columns=['country'], target='target')\n",
    "\n",
    "# 2. Aggregate by 'Brand'\n",
    "df_brand = create_aggregates(train_df, group_columns=['brand'], target='target')\n",
    "\n",
    "# 3. Aggregate by 'Drug_id'\n",
    "df_drug_id = create_aggregates(train_df, group_columns=['drug_id'], target='target')\n",
    "\n",
    "# 4. Aggregate by 'Country + Brand'\n",
    "df_country_brand = create_aggregates(train_df, group_columns=['country', 'brand'], target='target')\n",
    "\n",
    "# 5. Aggregate by 'Brand + Drug_id'\n",
    "df_brand_drug_id = create_aggregates(train_df, group_columns=['brand', 'drug_id'], target='target')\n",
    "\n",
    "# 6. Aggregate by 'Country + Drug_id'\n",
    "df_country_drug_id = create_aggregates(train_df, group_columns=['country', 'drug_id'], target='target')\n",
    "\n",
    "# 7. Aggregate by 'Country + Brand + Drug_id'\n",
    "df_country_brand_drug_id = create_aggregates(train_df, group_columns=['country', 'brand', 'drug_id'], target='target')\n",
    "\n",
    "# Map the aggregates into the original dataframe\n",
    "\n",
    "train_df['mean_country'] = train_df['country'].map(df_country['target_mean'])\n",
    "train_df['median_country'] = train_df['country'].map(df_country['target_median'])\n",
    "train_df['std_country'] = train_df['country'].map(df_country['target_std'])\n",
    "train_df['min_country'] = train_df['country'].map(df_country['target_min'])\n",
    "train_df['max_country'] = train_df['country'].map(df_country['target_max'])\n",
    "\n",
    "train_df['mean_brand'] = train_df['brand'].map(df_brand['target_mean'])\n",
    "train_df['median_brand'] = train_df['brand'].map(df_brand['target_median'])\n",
    "train_df['std_brand'] = train_df['brand'].map(df_brand['target_std'])\n",
    "train_df['min_brand'] = train_df['brand'].map(df_brand['target_min'])\n",
    "train_df['max_brand'] = train_df['brand'].map(df_brand['target_max'])\n",
    "\n",
    "train_df['mean_drug_id'] = train_df['drug_id'].map(df_drug_id['target_mean'])\n",
    "train_df['median_drug_id'] = train_df['drug_id'].map(df_drug_id['target_median'])\n",
    "train_df['std_drug_id'] = train_df['drug_id'].map(df_drug_id['target_std'])\n",
    "train_df['min_drug_id'] = train_df['drug_id'].map(df_drug_id['target_min'])\n",
    "train_df['max_drug_id'] = train_df['drug_id'].map(df_drug_id['target_max'])\n",
    "\n",
    "train_df['mean_country_brand'] = train_df[['country', 'brand']].apply(lambda x: df_country_brand.loc[tuple(x), 'target_mean'] if tuple(x) in df_country_brand.index else np.nan, axis=1)\n",
    "train_df['median_country_brand'] = train_df[['country', 'brand']].apply(lambda x: df_country_brand.loc[tuple(x), 'target_median'] if tuple(x) in df_country_brand.index else np.nan, axis=1)\n",
    "train_df['std_country_brand'] = train_df[['country', 'brand']].apply(lambda x: df_country_brand.loc[tuple(x), 'target_std'] if tuple(x) in df_country_brand.index else np.nan, axis=1)\n",
    "train_df['min_country_brand'] = train_df[['country', 'brand']].apply(lambda x: df_country_brand.loc[tuple(x), 'target_min'] if tuple(x) in df_country_brand.index else np.nan, axis=1)\n",
    "train_df['max_country_brand'] = train_df[['country', 'brand']].apply(lambda x: df_country_brand.loc[tuple(x), 'target_max'] if tuple(x) in df_country_brand.index else np.nan, axis=1)\n",
    "\n",
    "train_df['mean_brand_drug_id'] = train_df[['brand', 'drug_id']].apply(lambda x: df_brand_drug_id.loc[tuple(x), 'target_mean'] if tuple(x) in df_brand_drug_id.index else np.nan, axis=1)\n",
    "train_df['median_brand_drug_id'] = train_df[['brand', 'drug_id']].apply(lambda x: df_brand_drug_id.loc[tuple(x), 'target_median'] if tuple(x) in df_brand_drug_id.index else np.nan, axis=1)\n",
    "train_df['std_brand_drug_id'] = train_df[['brand', 'drug_id']].apply(lambda x: df_brand_drug_id.loc[tuple(x), 'target_std'] if tuple(x) in df_brand_drug_id.index else np.nan, axis=1)\n",
    "train_df['min_brand_drug_id'] = train_df[['brand', 'drug_id']].apply(lambda x: df_brand_drug_id.loc[tuple(x), 'target_min'] if tuple(x) in df_brand_drug_id.index else np.nan, axis=1)\n",
    "train_df['max_brand_drug_id'] = train_df[['brand', 'drug_id']].apply(lambda x: df_brand_drug_id.loc[tuple(x), 'target_max'] if tuple(x) in df_brand_drug_id.index else np.nan, axis=1)\n",
    "\n",
    "train_df['mean_country_drug_id'] = train_df[['country', 'drug_id']].apply(lambda x: df_country_drug_id.loc[tuple(x), 'target_mean'] if tuple(x) in df_country_drug_id.index else np.nan, axis=1)\n",
    "train_df['median_country_drug_id'] = train_df[['country', 'drug_id']].apply(lambda x: df_country_drug_id.loc[tuple(x), 'target_median'] if tuple(x) in df_country_drug_id.index else np.nan, axis=1)\n",
    "train_df['std_country_drug_id'] = train_df[['country', 'drug_id']].apply(lambda x: df_country_drug_id.loc[tuple(x), 'target_std'] if tuple(x) in df_country_drug_id.index else np.nan, axis=1)\n",
    "train_df['min_country_drug_id'] = train_df[['country', 'drug_id']].apply(lambda x: df_country_drug_id.loc[tuple(x), 'target_min'] if tuple(x) in df_country_drug_id.index else np.nan, axis=1)\n",
    "train_df['max_country_drug_id'] = train_df[['country', 'drug_id']].apply(lambda x: df_country_drug_id.loc[tuple(x), 'target_max'] if tuple(x) in df_country_drug_id.index else np.nan, axis=1)\n",
    "\n",
    "train_df['mean_country_brand_drug_id'] = train_df[['country', 'brand', 'drug_id']].apply(lambda x: df_country_brand_drug_id.loc[tuple(x), 'target_mean'] if tuple(x) in df_country_brand_drug_id.index else np.nan, axis=1)\n",
    "train_df['median_country_brand_drug_id'] = train_df[['country', 'brand', 'drug_id']].apply(lambda x: df_country_brand_drug_id.loc[tuple(x), 'target_median'] if tuple(x) in df_country_brand_drug_id.index else np.nan, axis=1)\n",
    "train_df['std_country_brand_drug_id'] = train_df[['country', 'brand', 'drug_id']].apply(lambda x: df_country_brand_drug_id.loc[tuple(x), 'target_std'] if tuple(x) in df_country_brand_drug_id.index else np.nan, axis=1)\n",
    "train_df['min_country_brand_drug_id'] = train_df[['country', 'brand', 'drug_id']].apply(lambda x: df_country_brand_drug_id.loc[tuple(x), 'target_min'] if tuple(x) in df_country_brand_drug_id.index else np.nan, axis=1)\n",
    "train_df['max_country_brand_drug_id'] = train_df[['country', 'brand', 'drug_id']].apply(lambda x: df_country_brand_drug_id.loc[tuple(x), 'target_max'] if tuple(x) in df_country_brand_drug_id.index else np.nan, axis=1)\n",
    "\n",
    "# Print the final dataframe with all aggregated features\n",
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_df_rolling[train_df_rolling['date'] < '2022-01-01']\n",
    "test_data = train_df_rolling[train_df_rolling['date'] >= '2022-01-01']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df.drop(['target', 'cluster_nl', 'launch_date', 'date', 'ind_launch_date', 'indication'], axis=1)\n",
    "# X_train = train_data.drop(['target', 'launch_date', 'date', 'ind_launch_date', 'indication'], axis=1)\n",
    "y_train = train_df['target']\n",
    "\n",
    "X_test = test_data.drop(['target', 'cluster_nl', 'launch_date', 'date', 'ind_launch_date', 'indication'], axis=1)\n",
    "# X_test = test_data.drop(['target', 'launch_date', 'date', 'ind_launch_date', 'indication'], axis=1)\n",
    "y_test = test_data['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model Architecture\n",
    "Define the architecture of the model using a suitable neural network for time series prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AE_MLP(nn.Module):\n",
    "    def __init__(self, num_columns, num_labels, hidden_units, dropout_rates):\n",
    "        super(AE_MLP, self).__init__()\n",
    "        \n",
    "        # Initial batch normalization\n",
    "        self.batch_norm0 = nn.BatchNorm1d(num_columns)\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder_noise = nn.Dropout(dropout_rates[0])\n",
    "        self.encoder_dense = nn.Linear(num_columns, hidden_units[0])\n",
    "        self.encoder_batch_norm = nn.BatchNorm1d(hidden_units[0])\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder_dropout = nn.Dropout(dropout_rates[1])\n",
    "        self.decoder_dense = nn.Linear(hidden_units[0], num_columns)\n",
    "        \n",
    "        # AE branch\n",
    "        self.x_ae_dense = nn.Linear(num_columns, hidden_units[1])\n",
    "        self.x_ae_batch_norm = nn.BatchNorm1d(hidden_units[1])\n",
    "        self.x_ae_dropout = nn.Dropout(dropout_rates[2])\n",
    "        self.out_ae_dense = nn.Linear(hidden_units[1], num_labels)\n",
    "        \n",
    "        # Concatenation and main branch\n",
    "        concat_input_dim = num_columns + hidden_units[0]\n",
    "        self.concat_batch_norm = nn.BatchNorm1d(concat_input_dim)\n",
    "        self.concat_dropout = nn.Dropout(dropout_rates[3])\n",
    "        \n",
    "        # Adjusted hidden layers\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        input_dim = concat_input_dim  # Start with concatenated dimension\n",
    "        for i in range(2, len(hidden_units)):\n",
    "            self.hidden_layers.append(nn.Linear(input_dim, hidden_units[i]))\n",
    "            self.hidden_layers.append(nn.BatchNorm1d(hidden_units[i]))\n",
    "            self.hidden_layers.append(nn.Dropout(dropout_rates[i + 2]))\n",
    "            input_dim = hidden_units[i]  # Update input_dim for next layer\n",
    "        \n",
    "        # Output layer\n",
    "        self.out_dense = nn.Linear(input_dim, num_labels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x0 = self.batch_norm0(x)\n",
    "        \n",
    "        # Encoder\n",
    "        encoder = self.encoder_noise(x0)\n",
    "        encoder = self.encoder_dense(encoder)\n",
    "        encoder = self.encoder_batch_norm(encoder)\n",
    "        encoder = F.silu(encoder)\n",
    "        \n",
    "        # Decoder\n",
    "        decoder = self.decoder_dropout(encoder)\n",
    "        decoder = self.decoder_dense(decoder)\n",
    "        \n",
    "        # AE branch\n",
    "        x_ae = self.x_ae_dense(decoder)\n",
    "        x_ae = self.x_ae_batch_norm(x_ae)\n",
    "        x_ae = F.silu(x_ae)\n",
    "        x_ae = self.x_ae_dropout(x_ae)\n",
    "        out_ae = torch.sigmoid(self.out_ae_dense(x_ae))\n",
    "        \n",
    "        # Main branch\n",
    "        x_concat = torch.cat([x0, encoder], dim=1)\n",
    "        x = self.concat_batch_norm(x_concat)\n",
    "        x = self.concat_dropout(x)\n",
    "        \n",
    "        for layer in self.hidden_layers:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                x = layer(x)\n",
    "                x = F.silu(x)\n",
    "            else:\n",
    "                x = layer(x)\n",
    "                \n",
    "        out = torch.sigmoid(self.out_dense(x))\n",
    "        \n",
    "        return decoder, out_ae, out\n",
    "\n",
    "# Example usage:\n",
    "# model = AE_MLP(num_columns, num_labels, hidden_units, dropout_rates)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "# criterion_decoder = nn.MSELoss()\n",
    "# criterion_ae_action = nn.BCEWithLogitsLoss()\n",
    "# criterion_action = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model\n",
    "Train the model on the entire dataset without using cross-validation. Save the best model using ModelCheckpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=None, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             multi_strategy=None, n_estimators=2000, n_jobs=-1,\n",
       "             num_parallel_tree=None, random_state=33, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBRegressor</label><div class=\"sk-toggleable__content\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=None, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             multi_strategy=None, n_estimators=2000, n_jobs=-1,\n",
       "             num_parallel_tree=None, random_state=33, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=None, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             multi_strategy=None, n_estimators=2000, n_jobs=-1,\n",
       "             num_parallel_tree=None, random_state=33, ...)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = XGBRegressor(n_estimators=2000, n_jobs=-1, random_state=33)\n",
    "model.fit(X_train, y_train, verbose=True) # Change verbose to True if you want to see it train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.0003\n",
      "R Score: 0.9999\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "print(f\"R Score: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Tuple\n",
    "\n",
    "def _CYME(df: pd.DataFrame) -> float:\n",
    "    \"\"\" Compute the CYME metric, that is 1/2(median(yearly error) + median(monthly error))\"\"\"\n",
    "\n",
    "    yearly_agg = df.groupby(\"cluster_nl\")[[\"target\", \"prediction\"]].sum().reset_index()\n",
    "    yearly_error = abs((yearly_agg[\"target\"] - yearly_agg[\"prediction\"])/yearly_agg[\"target\"]).median()\n",
    "\n",
    "    monthly_error = abs((df[\"target\"] - df[\"prediction\"])/df[\"target\"]).median()\n",
    "\n",
    "    return 1/2*(yearly_error + monthly_error)\n",
    "\n",
    "\n",
    "def _metric(df: pd.DataFrame) -> float:\n",
    "    \"\"\"Compute metric of submission.\n",
    "\n",
    "    :param df: Dataframe with target and 'prediction', and identifiers.\n",
    "    :return: Performance metric\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "\n",
    "    # Split 0 actuals - rest\n",
    "    zeros = df[df[\"zero_actuals\"] == 1]\n",
    "    recent = df[df[\"zero_actuals\"] == 0]\n",
    "\n",
    "    # weight for each group\n",
    "    zeros_weight = len(zeros)/len(df)\n",
    "    recent_weight = 1 - zeros_weight\n",
    "\n",
    "    # Compute CYME for each group\n",
    "    return round(recent_weight*_CYME(recent) + zeros_weight*min(1,_CYME(zeros)), 8)\n",
    "\n",
    "\n",
    "def compute_metric(submission: pd.DataFrame) -> Tuple[float, float]:\n",
    "    \"\"\"Compute metric.\n",
    "\n",
    "    :param submission: Prediction. Requires columns: ['cluster_nl', 'date', 'target', 'prediction']\n",
    "    :return: Performance metric.\n",
    "    \"\"\"\n",
    "\n",
    "    submission[\"date\"] = pd.to_datetime(submission[\"date\"])\n",
    "    submission = submission[['cluster_nl', 'date', 'target', 'prediction', 'zero_actuals']]\n",
    "\n",
    "    return _metric(submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance: 0.00486111\n"
     ]
    }
   ],
   "source": [
    "validation = test_data.copy()\n",
    "\n",
    "validation[\"prediction\"] = model.predict(validation[X_train.columns])\n",
    "\n",
    "# Assign column [\"zero_actuals\"] in the depending if in your\n",
    "# split the cluster_nl has already had actuals on train or not\n",
    "existing_clusters = train_data['cluster_nl'].unique()\n",
    "validation['zero_actuals'] = (~validation['cluster_nl'].isin(existing_clusters)).astype(int)\n",
    "\n",
    "print(\"Performance:\", compute_metric(validation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the Model\n",
    "Evaluate the model's performance on a validation set or using other suitable metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 1.0, 8)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv('SUBMISSION/Data Files/submission_data.csv')\n",
    "test_df.replace(-1.0, np.nan, inplace=True)\n",
    "test_df.replace('-1', np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[numeric_cols] = test_df[numeric_cols].fillna(test_df[numeric_cols].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_columns = ['launch_date', 'date', 'ind_launch_date']\n",
    "for col in date_columns:\n",
    "    test_df[col] = pd.to_datetime(test_df[col], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['launch_year'] = test_df['launch_date'].dt.year\n",
    "test_df['launch_month'] = test_df['launch_date'].dt.month\n",
    "test_df['date_year'] = test_df['date'].dt.year\n",
    "test_df['date_month'] = test_df['date'].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in label_enc_columns:\n",
    "    le = label_encoders[col]\n",
    "    test_df[col] = le.fit_transform(test_df[col].astype(str))\n",
    "    label_encoders[col] = le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_df.drop(['target', 'cluster_nl', 'launch_date', 'date', 'ind_launch_date', 'indication'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['date_str'] = test_df['date'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_data = pd.DataFrame({'date_str':test_df['date_str'], 'cluster_nl':test_df['cluster_nl'], 'prediction': y_pred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_csv = pd.read_csv('SUBMISSION/submission_template.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_csv.drop('prediction', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_csv['date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_csv = pd.merge(submission_csv, submission_data, left_on=['date', 'cluster_nl'], right_on=['date_str', 'cluster_nl'], how='left')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
